# 算法面经：阿里大模型后训练算法


## 🧠 【一面｜基础扎实 + 项目深挖】

1. **自我介绍**：建议控制在2分钟内，突出与大模型相关的经历✨
2. **高频八股**：大语言模型中的注意力机制？多头 vs 单头优势在哪？
3. **易混淆点**：LayerNorm和BatchNorm在训练时的梯度计算区别？
4. **架构对比**：当前主流LLM（如 Llama、Qwen、ChatGLM）在架构设计上有何异同？
5. **Prompt工程**：如何设计有效的提示词？有哪些实用技巧？
6. **微调重点**：LoRA是什么？原理 & 为什么它能高效微调大模型？
7. **RLHF必问**：RLHF训练流程？存在哪些潜在风险（比如奖励黑客、过拟合等）？
8. **幻觉问题**：大模型“胡说八道”的原因？有哪些缓解策略？
9. **In-Context Learning**：Zero-Shot/Few-Shot 的应用场景 & 局限性？
10. **评估方法论**：如何科学评估大模型性能？评估数据集怎么构造才靠谱？
11. **代码题**：lc 岛屿数量
 

## 🚀 【二面｜深度技术 + 工程落地】

1. **自我介绍**：面试官会结合简历追问细节！
2. **计算优化**：KV Cache 的内存瓶颈？推导多头注意力的时间/空间复杂度
3. **底层细节**：Tokenization 是如何工作的？BPE、WordPiece 有啥区别？
4. **模型压缩**：知识蒸馏在大模型压缩中的应用？效果如何？
5. **项目深挖**：你用过 Qwen？说说它的特点和优势（MoE？位置编码？训练数据？）
6. **训练调参**：微调 Qwen 时验证集 loss震荡，可能原因有哪些？（学习率？数据噪声？）
7. **参数量计算**：Qwen-14B 的 “14B” 怎么算出来的？推理时FLOPs大概多少？
8. **显存优化**：训练/推理显存不够？有哪些实用trick？（量化、Offload、FlashAttention…）
9. **前沿趋势**：聊聊 LLM 未来方向？多模态模型（如 Qwen-VL）如何融合图文信息？
10. **代码题**：lc最大乘积子数组
 